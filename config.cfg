# A simple config file to test experimenter

# General training meta-parameters
seed = 69
trainModel = true # if set to true, will fit model to "dataFname" tracking progress on "validFname"
dataFname = newsgroup_data/train.txt
validFname = newsgroup_data/valid.txt
graphFname = hybrid_vae_model
archType = hybrid-vae # piece-vae gaussian-vae hybrid-vae
outputDir = 20ng_hybridVAE/
dictFname = newsgroup_data/ng_lex.dict
miniBatchSize = 20

# general OGraph meta-parameters
n_hid_1 = 50 # num encoder hidden neurons
n_hid_2 = 50
n_lat = 50 # num latent variables
hidActivation = softsign
outputActivation = softmax
initType = gaussian(0,0.15)
outInitType = gaussian(0,0.15)
# variational autoencoder specific meta-parameters
vae_numPieces = 5
# alphas control interpolation btwn prior & posterior
vae_alpha_piece = 1.0 # leave at 1 as per discussion w/ Iulian
vae_alpha_mu = 0.5 # init at 0.5 -> these will be learned by the model
vae_alpha_sigma = 0.5 # init at 0.5 -> these will be learned by the model
vae_gamma = 1.0 # weights KL term
vae_beta = 1.0 # leave this at 1 (only useful for debugging)

# Optimization meta-parameters
loss_function = neg_log_loss # don't mess w/ this actually, as VAE uses the negative of this to get lower bound expectation
numEpochs = 50
errorMark = -1000
optimizer = sgd
lr = 0.1
norm_rescale = 15

# step-size scheduling meta-parameters
patience = 5 # how many failures tolerated before dividing lr by lr_div
lr_div = 2.0 # factor to divide lr by
epoch_bound = 1 # min num epochs before schedule applied
